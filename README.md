ğŸš€ Interview Copilot â€“ AI-Powered Interview Preparation Backend

Interview Copilot is a Spring Boot + Spring AI + Ollama powered backend application that helps generate custom interview questions and tailored answers based on a job description and a candidateâ€™s resume.
This project runs fully locally using Ollama models â€” no OpenAI API key required.

â­ Features

ğŸ” Generate role-specific interview questions using LLMs

ğŸ§  Generate tailored answers based on resume + job description

âš¡ Fully local inference using Ollama (supports llama3, phi3, mistral, qwen, etc.)

ğŸ—ï¸ REST APIs built with Spring Boot 3

ğŸ¤– AI integration using Spring AIâ€™s ChatClient

ğŸ“¦ Clean architecture: Controller â†’ Service â†’ LLM Layer

ğŸ“š Easily pluggable into React, Angular, or mobile frontend apps

ğŸ§© Ready for future extensions: mock chat interviewer, scoring engine, resume analyzer

ğŸ› ï¸ Tech Stack

Java 17+

Spring Boot 3.3.x

Spring AI 1.1.x

Ollama (local LLM runtime)

Maven

REST APIs

ğŸ“¦ Project Structure
interview-copilot/
â”‚
â”œâ”€â”€ src/main/java/com/imran/ai/interview/
â”‚   â”œâ”€â”€ InterviewCopilotApplication.java
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ AiConfig.java
â”‚   â”œâ”€â”€ controller/
â”‚   â”‚   â””â”€â”€ InterviewController.java
â”‚   â”œâ”€â”€ dto/
â”‚   â”‚   â””â”€â”€ InterviewDtos.java
â”‚   â””â”€â”€ service/
â”‚       â””â”€â”€ InterviewAiService.java
â”‚
â””â”€â”€ src/main/resources/
    â””â”€â”€ application.yml

ğŸ”§ Installation & Setup
1. Install Ollama

Download from:
â¡ï¸ https://ollama.com/download

Then pull your preferred model:

ollama pull llama3


Verify installation:

ollama list


You should see llama3 or any model you pulled.

Start the Ollama service (if not automatic):

ollama serve

2. Clone the repository
git clone https://github.com/<your-username>/interview-copilot.git
cd interview-copilot

3. Build and run the backend

Using Maven:

mvn clean install
mvn spring-boot:run


The server will start at:

http://localhost:8080

âš™ï¸ Configuration (application.yml)
spring:
  ai:
    ollama:
      base-url: http://localhost:11434
      chat:
        options:
          model: llama3
          temperature: 0.7
server:
  port: 8080

ğŸ“¡ API Endpoints
ğŸ“Œ 1. Generate Interview Questions

POST /api/interview/questions

Request Body:
{
  "jobDescription": "Senior Java Developer with Spring Boot, Microservices, Kafka...",
  "resumeText": "6+ years of experience in Java, Spring Boot, Kafka, AWS, Angular...",
  "numQuestions": 10
}

Response:
{
  "questions": [
    "1. Can you walk me through your experience with microservices?",
    "2. How have you used Kafka in real-time systems?",
    ...
  ]
}

ğŸ“Œ 2. Generate Answer for a Question

POST /api/interview/answers

Request Body:
{
  "jobDescription": "Java developer role...",
  "resumeText": "Spring Boot, Microservices, AWS...",
  "question": "Explain your experience with Kafka?"
}

Response:
{
  "answer": "In my previous project at Truist, I used Kafka for building event-driven pipelines..."
}

ğŸ§ª Testing the API in Postman

Set:

Body â†’ raw

Format â†’ JSON

Paste JSON and send the request.



ğŸš€ Future Enhancements

ğŸ—£ï¸ Mock interviewer chat mode

ğŸ“ Resume analyzer + score engine

ğŸ§® Interview difficulty levels

ğŸŒ Frontend UI (React/Angular/Flutter/iOS)

ğŸ¤ Multi-user interview sessions

ğŸ”’ JWT authentication for secure access

ğŸ™‹â€â™‚ï¸ Author

MD Imran
Java Full Stack Developer
Email: codiimran@gmail.com
